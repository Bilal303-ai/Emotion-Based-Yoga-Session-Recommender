{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install required libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install gradio\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:22:52.020378Z","iopub.execute_input":"2024-12-09T20:22:52.020869Z","iopub.status.idle":"2024-12-09T20:23:19.219827Z","shell.execute_reply.started":"2024-12-09T20:22:52.020821Z","shell.execute_reply":"2024-12-09T20:23:19.218594Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"markdown","source":"Create dictionaries to map labels to ids and vice versa","metadata":{}},{"cell_type":"code","source":"label2id = {\n    'neutral': 0,\n    'calm': 1,\n    'happy': 2,\n    'sad': 3,\n    'angry': 4,\n    'fearful': 5,\n    'disgust': 6,\n    'surprised': 7\n}\n\nid2label = {}\nfor key, value in label2id.items():\n    id2label.update({value: key})\n\nid2label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:23:38.144404Z","iopub.execute_input":"2024-12-09T20:23:38.145233Z","iopub.status.idle":"2024-12-09T20:23:38.152624Z","shell.execute_reply.started":"2024-12-09T20:23:38.145199Z","shell.execute_reply":"2024-12-09T20:23:38.151613Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{0: 'neutral',\n 1: 'calm',\n 2: 'happy',\n 3: 'sad',\n 4: 'angry',\n 5: 'fearful',\n 6: 'disgust',\n 7: 'surprised'}"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ndata = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        id = int(filename[7])\n        emotion = id2label[id-1]\n        data.append({\n            'path': file_path,\n            'emotion': emotion,\n            'id': id-1\n        })\n\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:23:43.800478Z","iopub.execute_input":"2024-12-09T20:23:43.800853Z","iopub.status.idle":"2024-12-09T20:23:48.096952Z","shell.execute_reply.started":"2024-12-09T20:23:43.800825Z","shell.execute_reply":"2024-12-09T20:23:48.096177Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:23:51.876241Z","iopub.execute_input":"2024-12-09T20:23:51.876618Z","iopub.status.idle":"2024-12-09T20:23:51.895959Z","shell.execute_reply.started":"2024-12-09T20:23:51.876586Z","shell.execute_reply":"2024-12-09T20:23:51.894991Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                path    emotion  id\n0  /kaggle/input/ravdess-emotional-speech-audio/A...  surprised   7\n1  /kaggle/input/ravdess-emotional-speech-audio/A...    neutral   0\n2  /kaggle/input/ravdess-emotional-speech-audio/A...    disgust   6\n3  /kaggle/input/ravdess-emotional-speech-audio/A...    disgust   6\n4  /kaggle/input/ravdess-emotional-speech-audio/A...    neutral   0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>emotion</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>surprised</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>neutral</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>disgust</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>disgust</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>neutral</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df['emotion'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:24:03.521087Z","iopub.execute_input":"2024-12-09T20:24:03.521950Z","iopub.status.idle":"2024-12-09T20:24:03.540293Z","shell.execute_reply.started":"2024-12-09T20:24:03.521911Z","shell.execute_reply":"2024-12-09T20:24:03.539198Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"emotion\nsurprised    384\ndisgust      384\nfearful      384\nsad          384\nhappy        384\ncalm         384\nangry        384\nneutral      192\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"#### Split the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Audio, Dataset\n#Create hugging face audio dataset\ndataset = Dataset.from_dict({\"audio\": df['path'], \"label\": df['id']}).cast_column(\"audio\", Audio())\ndataset = dataset.train_test_split(seed=42, shuffle=True, test_size=0.2)\ndataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:24:12.312459Z","iopub.execute_input":"2024-12-09T20:24:12.313441Z","iopub.status.idle":"2024-12-09T20:24:26.163234Z","shell.execute_reply.started":"2024-12-09T20:24:12.313381Z","shell.execute_reply":"2024-12-09T20:24:26.162286Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'audio': {'path': '/kaggle/input/ravdess-emotional-speech-audio/Actor_03/03-01-08-01-01-02-03.wav',\n  'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n         0.00000000e+00, 0.00000000e+00, 3.05175781e-05]),\n  'sampling_rate': 48000},\n 'label': 7}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### Listen to few examples using gradio interface","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\ndef generate_audio():\n    example = dataset[\"train\"].shuffle()[0]\n    audio = example[\"audio\"]\n    return (\n        audio[\"sampling_rate\"],\n        audio[\"array\"],\n    ), id2label[example[\"label\"]]\n\nwith gr.Blocks() as demo:\n    with gr.Column():\n        for _ in range(4):\n            audio, label = generate_audio()\n            output = gr.Audio(audio, label=label)\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:24:38.523853Z","iopub.execute_input":"2024-12-09T20:24:38.525014Z","iopub.status.idle":"2024-12-09T20:24:42.617897Z","shell.execute_reply.started":"2024-12-09T20:24:38.524975Z","shell.execute_reply":"2024-12-09T20:24:42.616950Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gradio/processing_utils.py:738: UserWarning: Trying to convert audio automatically from float64 to 16-bit int format.\n  warnings.warn(warning.format(data.dtype))\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://0c892292e917aba7a0.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://0c892292e917aba7a0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"#### Preprocess the data","metadata":{}},{"cell_type":"markdown","source":"I will use DistilHuBERT model for fine tuning","metadata":{}},{"cell_type":"code","source":"from transformers import AutoFeatureExtractor\n\nmodel_id = \"ntu-spml/distilhubert\"\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\n    model_id, do_normalize=True, return_attention_mask=True\n)\n\nsampling_rate = feature_extractor.sampling_rate\nprint(\"Model's sample rate: \", sampling_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:24:58.641588Z","iopub.execute_input":"2024-12-09T20:24:58.641954Z","iopub.status.idle":"2024-12-09T20:24:58.808183Z","shell.execute_reply.started":"2024-12-09T20:24:58.641925Z","shell.execute_reply":"2024-12-09T20:24:58.807186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f609804c1e4294a368cfe1580477d6"}},"metadata":{}},{"name":"stdout","text":"Model's sample rate:  16000\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Since sampling rates of dataset and model are different, we will need to convert the sample rate of our samples to 16000 Hz.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\ndataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:25:06.409338Z","iopub.execute_input":"2024-12-09T20:25:06.409719Z","iopub.status.idle":"2024-12-09T20:25:06.437884Z","shell.execute_reply.started":"2024-12-09T20:25:06.409691Z","shell.execute_reply":"2024-12-09T20:25:06.436616Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'audio': {'path': '/kaggle/input/ravdess-emotional-speech-audio/Actor_03/03-01-08-01-01-02-03.wav',\n  'array': array([-3.04453351e-10,  3.02634362e-10, -2.81943358e-10, ...,\n         -1.75207053e-04, -6.04685229e-05,  2.10753860e-05]),\n  'sampling_rate': 16000},\n 'label': 7}"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def preprocess_function(examples):\n    \"\"\"Prepare the dataset for training\"\"\"\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays,\n        sampling_rate = feature_extractor.sampling_rate,\n        return_attention_mask = True\n    )\n    return inputs\n\ndataset_enc = dataset.map(\n    preprocess_function,\n    remove_columns=['audio'],\n    batched=True,\n    batch_size=100,\n    num_proc=1\n)\n\ndataset_enc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:25:11.265399Z","iopub.execute_input":"2024-12-09T20:25:11.265821Z","iopub.status.idle":"2024-12-09T20:25:48.453816Z","shell.execute_reply.started":"2024-12-09T20:25:11.265786Z","shell.execute_reply":"2024-12-09T20:25:48.452942Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2304 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ab56974ef84867b29f0ec5359f82f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/576 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3e434018654126bc5b30f89e12ad9f"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_values', 'attention_mask'],\n        num_rows: 2304\n    })\n    test: Dataset({\n        features: ['label', 'input_values', 'attention_mask'],\n        num_rows: 576\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"#### Fine-tuning the model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForAudioClassification\n\nnum_labels = len(id2label)\nmodel = AutoModelForAudioClassification.from_pretrained(\n    model_id,\n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:25:52.899192Z","iopub.execute_input":"2024-12-09T20:25:52.900159Z","iopub.status.idle":"2024-12-09T20:25:53.805249Z","shell.execute_reply.started":"2024-12-09T20:25:52.900119Z","shell.execute_reply":"2024-12-09T20:25:53.804350Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a581b10a82a14187bbd28a9d44bf3d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/94.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8fab3e542744548fab3ca745ad6eb8"}},"metadata":{}},{"name":"stderr","text":"Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nmodel_name = model_id.split(\"/\")[-1]\nbatch_size = 8\ngradient_accumulation_steps = 1\nnum_train_epochs = 7\n\ntraining_args = TrainingArguments(\n    f\"{model_name}-finetuned-ravdess\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=False,\n    report_to='none'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:27:33.861308Z","iopub.execute_input":"2024-12-09T20:27:33.862190Z","iopub.status.idle":"2024-12-09T20:27:33.900126Z","shell.execute_reply.started":"2024-12-09T20:27:33.862154Z","shell.execute_reply":"2024-12-09T20:27:33.899065Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\nmetric = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:27:38.941313Z","iopub.execute_input":"2024-12-09T20:27:38.941713Z","iopub.status.idle":"2024-12-09T20:27:39.243100Z","shell.execute_reply.started":"2024-12-09T20:27:38.941680Z","shell.execute_reply":"2024-12-09T20:27:39.241847Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=dataset_enc[\"train\"],\n    eval_dataset=dataset_enc[\"test\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:27:43.397300Z","iopub.execute_input":"2024-12-09T20:27:43.397681Z","iopub.status.idle":"2024-12-09T20:50:29.619218Z","shell.execute_reply.started":"2024-12-09T20:27:43.397647Z","shell.execute_reply":"2024-12-09T20:50:29.618220Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/360634443.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1008' max='1008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1008/1008 22:43, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.551600</td>\n      <td>1.462943</td>\n      <td>0.519097</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.885800</td>\n      <td>0.940370</td>\n      <td>0.713542</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.484400</td>\n      <td>0.518282</td>\n      <td>0.857639</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.199900</td>\n      <td>0.281870</td>\n      <td>0.932292</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.162000</td>\n      <td>0.183065</td>\n      <td>0.953125</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.083700</td>\n      <td>0.130776</td>\n      <td>0.963542</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.057400</td>\n      <td>0.118378</td>\n      <td>0.972222</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1008, training_loss=0.6410924392560172, metrics={'train_runtime': 1365.6103, 'train_samples_per_second': 11.81, 'train_steps_per_second': 0.738, 'total_flos': 1.6137221209918848e+17, 'train_loss': 0.6410924392560172, 'epoch': 7.0})"},"metadata":{}}],"execution_count":19}]}